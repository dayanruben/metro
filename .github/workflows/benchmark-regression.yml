name: Benchmark Regression

on:
  push:
    branches: [main]
    paths:
      - 'compiler/**'
      - 'runtime/**'
      - 'benchmark/**'
  pull_request:
    types: [labeled]

permissions:
  contents: write  # For pushing to gh-pages
  pull-requests: write  # For commenting on PRs

jobs:
  benchmark:
    name: "Multiplatform Startup Benchmark"
    # Run on push to main, or on PRs with 'run-benchmarks' label
    if: github.event_name == 'push' || (github.event_name == 'pull_request' && github.event.label.name == 'run-benchmarks')
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v6
        with:
          fetch-depth: 0

      - name: Determine refs to benchmark
        id: refs
        run: |
          if [[ "${{ github.event_name }}" == "push" ]]; then
            # On push to main: compare HEAD~1 (baseline) vs HEAD (current)
            echo "baseline_ref=HEAD~1" >> $GITHUB_OUTPUT
            echo "current_ref=HEAD" >> $GITHUB_OUTPUT
            echo "baseline_name=main~1" >> $GITHUB_OUTPUT
            echo "current_name=main" >> $GITHUB_OUTPUT
          else
            # On PR: compare main (baseline) vs PR branch (current)
            echo "baseline_ref=origin/main" >> $GITHUB_OUTPUT
            echo "current_ref=${{ github.event.pull_request.head.sha }}" >> $GITHUB_OUTPUT
            echo "baseline_name=main" >> $GITHUB_OUTPUT
            echo "current_name=PR #${{ github.event.pull_request.number }}" >> $GITHUB_OUTPUT
          fi

      - name: Configure JDK
        uses: actions/setup-java@v5
        with:
          distribution: 'zulu'
          java-version-file: .github/workflows/.java-version

      - name: Setup Gradle
        uses: gradle/actions/setup-gradle@v5
        with:
          cache-read-only: true
          gradle-home-cache-strict-match: true
          cache-encryption-key: ${{ secrets.GRADLE_ENCRYPTION_KEY }}

      # Run baseline benchmark
      - name: Checkout baseline (${{ steps.refs.outputs.baseline_name }})
        run: git checkout ${{ steps.refs.outputs.baseline_ref }}

      - name: Run baseline benchmark
        run: |
          cd benchmark
          ./run_startup_benchmarks.sh jvm --modes metro --count 500

      - name: Save baseline results
        id: baseline
        run: |
          # JMH produces results.json
          json_path=$(find benchmark/startup-benchmark-results -name "results.json" 2>/dev/null | head -1)
          if [ -z "$json_path" ]; then
            echo "No baseline results.json found"
            exit 1
          fi

          # Extract score
          score=$(python3 -c "import json; data=json.load(open('$json_path')); print(data[0]['primaryMetric']['score'])")
          echo "score=$score" >> $GITHUB_OUTPUT

          # Copy to temp location
          mkdir -p /tmp/benchmark-baseline
          cp "$json_path" /tmp/benchmark-baseline/results.json

          # Create modified JSON with renamed benchmark for charting
          python3 -c "
          import json
          with open('$json_path') as f:
              data = json.load(f)
          data[0]['benchmark'] = 'baseline'
          with open('/tmp/benchmark-baseline/baseline.json', 'w') as f:
              json.dump(data, f)
          "
          echo "json_path=/tmp/benchmark-baseline/baseline.json" >> $GITHUB_OUTPUT

          # Extract binary metrics from class-metrics.json
          class_metrics=$(find benchmark/startup-benchmark-results -name "class-metrics.json" 2>/dev/null | head -1)
          if [ -n "$class_metrics" ] && [ -f "$class_metrics" ]; then
            cp "$class_metrics" /tmp/benchmark-baseline/class-metrics.json
            fields=$(python3 -c "import json; data=json.load(open('$class_metrics')); print(data.get('fields', 0))")
            methods=$(python3 -c "import json; data=json.load(open('$class_metrics')); print(data.get('methods', 0))")
            size_bytes=$(python3 -c "import json; data=json.load(open('$class_metrics')); print(data.get('total_size_bytes', 0))")
            size_kb=$(python3 -c "print(f'{$size_bytes / 1024:.1f}')")
            echo "fields=$fields" >> $GITHUB_OUTPUT
            echo "methods=$methods" >> $GITHUB_OUTPUT
            echo "size_kb=$size_kb" >> $GITHUB_OUTPUT
          else
            echo "No class-metrics.json found, skipping binary metrics"
            echo "fields=0" >> $GITHUB_OUTPUT
            echo "methods=0" >> $GITHUB_OUTPUT
            echo "size_kb=0" >> $GITHUB_OUTPUT
          fi

      # Run current benchmark
      - name: Checkout current (${{ steps.refs.outputs.current_name }})
        run: git checkout ${{ steps.refs.outputs.current_ref }}

      - name: Run current benchmark
        run: |
          cd benchmark
          # Clean previous results
          rm -rf startup-benchmark-results
          ./run_startup_benchmarks.sh jvm --modes metro --count 500

      - name: Save current results
        id: current
        run: |
          # JMH produces results.json
          json_path=$(find benchmark/startup-benchmark-results -name "results.json" 2>/dev/null | head -1)
          if [ -z "$json_path" ]; then
            echo "No current results.json found"
            exit 1
          fi

          # Extract score
          score=$(python3 -c "import json; data=json.load(open('$json_path')); print(data[0]['primaryMetric']['score'])")
          echo "score=$score" >> $GITHUB_OUTPUT

          # Copy to temp location
          mkdir -p /tmp/benchmark-current
          cp "$json_path" /tmp/benchmark-current/results.json

          # Create modified JSON with renamed benchmark for charting
          python3 -c "
          import json
          with open('$json_path') as f:
              data = json.load(f)
          data[0]['benchmark'] = 'current'
          with open('/tmp/benchmark-current/current.json', 'w') as f:
              json.dump(data, f)
          "
          echo "json_path=/tmp/benchmark-current/current.json" >> $GITHUB_OUTPUT

          # Extract binary metrics from class-metrics.json
          class_metrics=$(find benchmark/startup-benchmark-results -name "class-metrics.json" 2>/dev/null | head -1)
          if [ -n "$class_metrics" ] && [ -f "$class_metrics" ]; then
            cp "$class_metrics" /tmp/benchmark-current/class-metrics.json
            fields=$(python3 -c "import json; data=json.load(open('$class_metrics')); print(data.get('fields', 0))")
            methods=$(python3 -c "import json; data=json.load(open('$class_metrics')); print(data.get('methods', 0))")
            size_bytes=$(python3 -c "import json; data=json.load(open('$class_metrics')); print(data.get('total_size_bytes', 0))")
            size_kb=$(python3 -c "print(f'{$size_bytes / 1024:.1f}')")
            echo "fields=$fields" >> $GITHUB_OUTPUT
            echo "methods=$methods" >> $GITHUB_OUTPUT
            echo "size_kb=$size_kb" >> $GITHUB_OUTPUT
          else
            echo "No class-metrics.json found, skipping binary metrics"
            echo "fields=0" >> $GITHUB_OUTPUT
            echo "methods=0" >> $GITHUB_OUTPUT
            echo "size_kb=0" >> $GITHUB_OUTPUT
          fi

      - name: Calculate delta and check regression
        id: compare
        run: |
          baseline=${{ steps.baseline.outputs.score }}
          current=${{ steps.current.outputs.score }}

          delta_pct=$(python3 -c "
          baseline = $baseline
          current = $current
          delta = ((current - baseline) / baseline) * 100
          print(f'{delta:.2f}')
          ")

          echo "delta_pct=$delta_pct" >> $GITHUB_OUTPUT

          # Check if regression (>5% slower)
          is_regression=$(python3 -c "print('true' if $delta_pct > 5 else 'false')")
          echo "is_regression=$is_regression" >> $GITHUB_OUTPUT

      - name: Add summary
        run: |
          baseline_score=${{ steps.baseline.outputs.score }}
          current_score=${{ steps.current.outputs.score }}
          delta_pct=${{ steps.compare.outputs.delta_pct }}
          is_regression=${{ steps.compare.outputs.is_regression }}

          # Binary metrics
          baseline_fields=${{ steps.baseline.outputs.fields }}
          baseline_methods=${{ steps.baseline.outputs.methods }}
          baseline_size_kb=${{ steps.baseline.outputs.size_kb }}
          current_fields=${{ steps.current.outputs.fields }}
          current_methods=${{ steps.current.outputs.methods }}
          current_size_kb=${{ steps.current.outputs.size_kb }}

          echo "## Multiplatform Startup Benchmark" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [[ "$is_regression" == "true" ]]; then
            echo "### ⚠️ Regression Detected" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          fi

          echo "| Ref | Score (ms/op) |" >> $GITHUB_STEP_SUMMARY
          echo "|-----|---------------|" >> $GITHUB_STEP_SUMMARY
          echo "| ${{ steps.refs.outputs.baseline_name }} (baseline) | ${baseline_score} |" >> $GITHUB_STEP_SUMMARY
          echo "| ${{ steps.refs.outputs.current_name }} (current) | ${current_score} |" >> $GITHUB_STEP_SUMMARY
          echo "| **Delta** | **${delta_pct}%** |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Add binary metrics table if we have data
          if [[ "$baseline_fields" != "0" ]] || [[ "$current_fields" != "0" ]]; then
            # Calculate deltas for binary metrics
            fields_delta=$((current_fields - baseline_fields))
            methods_delta=$((current_methods - baseline_methods))
            size_delta=$(python3 -c "print(f'{$current_size_kb - $baseline_size_kb:.1f}')")

            # Format with sign
            fields_sign=""
            if [ "$fields_delta" -gt 0 ]; then fields_sign="+"; fi
            methods_sign=""
            if [ "$methods_delta" -gt 0 ]; then methods_sign="+"; fi
            size_sign=""
            if (( $(echo "$current_size_kb > $baseline_size_kb" | bc -l) )); then size_sign="+"; fi

            echo "### Binary Metrics (Pre-Minification)" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "| Metric | Baseline | Current | Delta |" >> $GITHUB_STEP_SUMMARY
            echo "|--------|----------|---------|-------|" >> $GITHUB_STEP_SUMMARY
            echo "| Fields | ${baseline_fields} | ${current_fields} | ${fields_sign}${fields_delta} |" >> $GITHUB_STEP_SUMMARY
            echo "| Methods | ${baseline_methods} | ${current_methods} | ${methods_sign}${methods_delta} |" >> $GITHUB_STEP_SUMMARY
            echo "| Size (KB) | ${baseline_size_kb} | ${current_size_kb} | ${size_sign}${size_delta} |" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          fi

          echo "_Both benchmarks run on the same machine. The delta accounts for hardware variance._" >> $GITHUB_STEP_SUMMARY

      - name: Comment on PR if regression
        if: github.event_name == 'pull_request' && steps.compare.outputs.is_regression == 'true'
        uses: actions/github-script@v8
        with:
          script: |
            const baselineFields = '${{ steps.baseline.outputs.fields }}';
            const currentFields = '${{ steps.current.outputs.fields }}';
            const baselineMethods = '${{ steps.baseline.outputs.methods }}';
            const currentMethods = '${{ steps.current.outputs.methods }}';
            const baselineSize = '${{ steps.baseline.outputs.size_kb }}';
            const currentSize = '${{ steps.current.outputs.size_kb }}';

            let binaryMetrics = '';
            if (baselineFields !== '0' || currentFields !== '0') {
              const fieldsDelta = parseInt(currentFields) - parseInt(baselineFields);
              const methodsDelta = parseInt(currentMethods) - parseInt(baselineMethods);
              const sizeDelta = (parseFloat(currentSize) - parseFloat(baselineSize)).toFixed(1);
              const formatDelta = (d) => d > 0 ? `+${d}` : `${d}`;

              binaryMetrics = `
            ### Binary Metrics (Pre-Minification)

            | Metric | Baseline | Current | Delta |
            |--------|----------|---------|-------|
            | Fields | ${baselineFields} | ${currentFields} | ${formatDelta(fieldsDelta)} |
            | Methods | ${baselineMethods} | ${currentMethods} | ${formatDelta(methodsDelta)} |
            | Size (KB) | ${baselineSize} | ${currentSize} | ${formatDelta(sizeDelta)} |
            `;
            }

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: `## ⚠️ Benchmark Regression Detected

            | Ref | Score (ms/op) |
            |-----|---------------|
            | main (baseline) | ${{ steps.baseline.outputs.score }} |
            | This PR (current) | ${{ steps.current.outputs.score }} |
            | **Delta** | **${{ steps.compare.outputs.delta_pct }}%** |
            ${binaryMetrics}
            The startup benchmark shows a >5% regression. Please investigate.

            _Both benchmarks were run on the same machine to ensure a fair comparison._`
            })

      - name: Upload results
        uses: actions/upload-artifact@v6
        with:
          name: startup-benchmark-results
          path: |
            /tmp/benchmark-baseline/
            /tmp/benchmark-current/
          retention-days: 7

  build-benchmark:
    name: "Build Time Benchmark"
    # Run on push to main, or on PRs with 'run-benchmarks' label
    if: github.event_name == 'push' || (github.event_name == 'pull_request' && github.event.label.name == 'run-benchmarks')
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v6
        with:
          fetch-depth: 0

      - name: Determine refs to benchmark
        id: refs
        run: |
          if [[ "${{ github.event_name }}" == "push" ]]; then
            echo "baseline_ref=HEAD~1" >> $GITHUB_OUTPUT
            echo "current_ref=HEAD" >> $GITHUB_OUTPUT
            echo "baseline_name=main~1" >> $GITHUB_OUTPUT
            echo "current_name=main" >> $GITHUB_OUTPUT
          else
            echo "baseline_ref=origin/main" >> $GITHUB_OUTPUT
            echo "current_ref=${{ github.event.pull_request.head.sha }}" >> $GITHUB_OUTPUT
            echo "baseline_name=main" >> $GITHUB_OUTPUT
            echo "current_name=PR #${{ github.event.pull_request.number }}" >> $GITHUB_OUTPUT
          fi

      - name: Configure JDK
        uses: actions/setup-java@v5
        with:
          distribution: 'zulu'
          java-version-file: .github/workflows/.java-version

      - name: Setup Gradle
        uses: gradle/actions/setup-gradle@v5
        with:
          cache-read-only: true
          gradle-home-cache-strict-match: true
          cache-encryption-key: ${{ secrets.GRADLE_ENCRYPTION_KEY }}

      - name: Get cache key for gradle-profiler
        id: gradle-profiler-cache-key
        run: echo "week=$(date +%Y-W%V)" >> $GITHUB_OUTPUT

      - name: Cache gradle-profiler
        uses: actions/cache@v5
        with:
          path: tmp/gradle-profiler-source/build/install/gradle-profiler
          key: gradle-profiler-${{ runner.os }}-${{ steps.gradle-profiler-cache-key.outputs.week }}
          restore-keys: gradle-profiler-${{ runner.os }}-

      - name: Install gradle-profiler
        run: ./benchmark/install-gradle-profiler.sh

      # Run baseline benchmark
      - name: Checkout baseline (${{ steps.refs.outputs.baseline_name }})
        run: git checkout ${{ steps.refs.outputs.baseline_ref }}

      - name: Run baseline build benchmark
        run: |
          cd benchmark
          ./run_benchmarks.sh single --ref HEAD --modes metro --scenarios abi_change,raw_compilation 500

      - name: Save baseline results
        id: baseline
        run: |
          # Find the benchmark results CSV
          csv_file=$(find benchmark/benchmark-results -name "benchmark.csv" 2>/dev/null | head -1)
          if [ -z "$csv_file" ]; then
            echo "No baseline benchmark.csv found"
            exit 1
          fi

          # Extract mean build time from measured builds
          # Gradle profiler CSV format: scenario,value (e.g., "measured build 1,1234")
          mean_ms=$(python3 -c "
          import csv
          with open('$csv_file') as f:
              reader = csv.reader(f)
              values = [float(row[1]) for row in reader if row[0].startswith('measured build')]
              print(sum(values) / len(values) if values else 0)
          ")
          echo "score=$mean_ms" >> $GITHUB_OUTPUT

          # Copy results
          mkdir -p /tmp/build-benchmark-baseline
          cp -r benchmark/benchmark-results/* /tmp/build-benchmark-baseline/

          # Create JSON for benchmark-action (customSmallerIsBetter format)
          python3 -c "
          import json
          result = [{
              'name': 'Build Time (baseline)',
              'unit': 'ms',
              'value': $mean_ms
          }]
          with open('/tmp/build-benchmark-baseline/baseline.json', 'w') as f:
              json.dump(result, f)
          "
          echo "json_path=/tmp/build-benchmark-baseline/baseline.json" >> $GITHUB_OUTPUT

      # Run current benchmark
      - name: Checkout current (${{ steps.refs.outputs.current_name }})
        run: git checkout ${{ steps.refs.outputs.current_ref }}

      - name: Run current build benchmark
        run: |
          cd benchmark
          rm -rf benchmark-results
          ./run_benchmarks.sh single --ref HEAD --modes metro --scenarios abi_change,raw_compilation 500

      - name: Save current results
        id: current
        run: |
          csv_file=$(find benchmark/benchmark-results -name "benchmark.csv" 2>/dev/null | head -1)
          if [ -z "$csv_file" ]; then
            echo "No current benchmark.csv found"
            exit 1
          fi

          # Gradle profiler CSV format: scenario,value (e.g., "measured build 1,1234")
          mean_ms=$(python3 -c "
          import csv
          with open('$csv_file') as f:
              reader = csv.reader(f)
              values = [float(row[1]) for row in reader if row[0].startswith('measured build')]
              print(sum(values) / len(values) if values else 0)
          ")
          echo "score=$mean_ms" >> $GITHUB_OUTPUT

          mkdir -p /tmp/build-benchmark-current
          cp -r benchmark/benchmark-results/* /tmp/build-benchmark-current/

          python3 -c "
          import json
          result = [{
              'name': 'Build Time (current)',
              'unit': 'ms',
              'value': $mean_ms
          }]
          with open('/tmp/build-benchmark-current/current.json', 'w') as f:
              json.dump(result, f)
          "
          echo "json_path=/tmp/build-benchmark-current/current.json" >> $GITHUB_OUTPUT

      - name: Calculate delta and check regression
        id: compare
        run: |
          baseline=${{ steps.baseline.outputs.score }}
          current=${{ steps.current.outputs.score }}

          delta_pct=$(python3 -c "
          baseline = $baseline
          current = $current
          delta = ((current - baseline) / baseline) * 100 if baseline != 0 else 0
          print(f'{delta:.2f}')
          ")
          echo "delta_pct=$delta_pct" >> $GITHUB_OUTPUT

          is_regression=$(python3 -c "print('true' if float('$delta_pct') > 5 else 'false')")
          echo "is_regression=$is_regression" >> $GITHUB_OUTPUT

      - name: Add summary
        run: |
          baseline_score=${{ steps.baseline.outputs.score }}
          current_score=${{ steps.current.outputs.score }}
          delta_pct=${{ steps.compare.outputs.delta_pct }}
          is_regression=${{ steps.compare.outputs.is_regression }}

          # Convert ms to seconds for readability
          baseline_s=$(python3 -c "print(f'{$baseline_score / 1000:.2f}')")
          current_s=$(python3 -c "print(f'{$current_score / 1000:.2f}')")

          echo "## Build Time Benchmark" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [[ "$is_regression" == "true" ]]; then
            echo "### ⚠️ Regression Detected" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          fi

          echo "| Ref | Build Time |" >> $GITHUB_STEP_SUMMARY
          echo "|-----|------------|" >> $GITHUB_STEP_SUMMARY
          echo "| ${{ steps.refs.outputs.baseline_name }} (baseline) | ${baseline_s}s |" >> $GITHUB_STEP_SUMMARY
          echo "| ${{ steps.refs.outputs.current_name }} (current) | ${current_s}s |" >> $GITHUB_STEP_SUMMARY
          echo "| **Delta** | **${delta_pct}%** |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "_Both benchmarks run on the same machine. The delta accounts for hardware variance._" >> $GITHUB_STEP_SUMMARY

      - name: Comment on PR if regression
        if: github.event_name == 'pull_request' && steps.compare.outputs.is_regression == 'true'
        uses: actions/github-script@v8
        with:
          script: |
            const baseline_s = (${{ steps.baseline.outputs.score }} / 1000).toFixed(2);
            const current_s = (${{ steps.current.outputs.score }} / 1000).toFixed(2);
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: `## ⚠️ Build Time Regression Detected

            | Ref | Build Time |
            |-----|------------|
            | main (baseline) | ${baseline_s}s |
            | This PR (current) | ${current_s}s |
            | **Delta** | **${{ steps.compare.outputs.delta_pct }}%** |

            The build benchmark shows a >5% regression. Please investigate.

            _Both benchmarks were run on the same machine to ensure a fair comparison._`
            })

      - name: Upload results
        uses: actions/upload-artifact@v6
        with:
          name: build-benchmark-results
          path: |
            /tmp/build-benchmark-baseline/
            /tmp/build-benchmark-current/
          retention-days: 7

  publish-benchmarks:
    name: "Publish Benchmark Results"
    # Only run on push to main, after both benchmark jobs complete
    if: github.event_name == 'push'
    needs: [benchmark, build-benchmark]
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v6

      - name: Download startup benchmark results
        uses: actions/download-artifact@v7
        with:
          name: startup-benchmark-results
          path: /tmp/startup-results

      - name: Download build benchmark results
        uses: actions/download-artifact@v7
        with:
          name: build-benchmark-results
          path: /tmp/build-results

      # Store all results sequentially, only pushing on the final one
      - name: Store startup baseline result
        uses: benchmark-action/github-action-benchmark@v1
        with:
          name: Startup (baseline)
          tool: 'jmh'
          output-file-path: /tmp/startup-results/benchmark-baseline/baseline.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          auto-push: false
          gh-pages-branch: gh-pages
          benchmark-data-dir-path: dev/bench

      - name: Store startup current result
        uses: benchmark-action/github-action-benchmark@v1
        with:
          name: Startup (current)
          tool: 'jmh'
          output-file-path: /tmp/startup-results/benchmark-current/current.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          auto-push: false
          skip-fetch-gh-pages: true
          gh-pages-branch: gh-pages
          benchmark-data-dir-path: dev/bench

      - name: Store build baseline result
        uses: benchmark-action/github-action-benchmark@v1
        with:
          name: Build Time (baseline)
          tool: 'customSmallerIsBetter'
          output-file-path: /tmp/build-results/build-benchmark-baseline/baseline.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          auto-push: false
          skip-fetch-gh-pages: true
          gh-pages-branch: gh-pages
          benchmark-data-dir-path: dev/bench/build

      - name: Store build current result
        uses: benchmark-action/github-action-benchmark@v1
        with:
          name: Build Time (current)
          tool: 'customSmallerIsBetter'
          output-file-path: /tmp/build-results/build-benchmark-current/current.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          auto-push: true
          skip-fetch-gh-pages: true
          gh-pages-branch: gh-pages
          benchmark-data-dir-path: dev/bench/build
