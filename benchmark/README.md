# Metro Benchmark Project

This is a large-scale benchmark project for testing Metro's performance. It contains a configurable number of modules
(default: 500) organized in a realistic architecture with extensive use of `@ContributesBinding` and `@ContributesMultibinding` annotations.

## Architecture

The project is organized into three layers:

- **Core layer** (~16% of total): Fundamental utilities, data models, networking, platform abstractions
- **Features layer** (~70% of total): Business logic features like auth, user management, content, social, commerce,
  analytics
- **App layer** (~14% of total): UI components, navigation, integration glue, and dependency wiring

## Module Structure

Each module contributes 1-10 bindings using:

- `@ContributesBinding` for interface implementations
- `@ContributesMultibinding` for plugin-style extensions and initializers
- Realistic inter-module dependencies following the layered architecture
- 5 subcomponents for hierarchical scoping

## Generation

The entire project is generated by the `generate-projects.main.kts` script, which can be re-run to regenerate modules
with different parameters. This makes it easy to experiment with different scales and configurations.

The script supports multiple modes and configurable module counts:
- **Metro mode** (`--mode metro`): Uses Metro for dependency injection with metro interop
- **Vanilla mode** (`--mode vanilla`): Pure Kotlin with no DI framework (true baseline)
- **Metro-NOOP mode** (`--mode metro_noop`): Metro compiler plugin applied but no Metro annotations (measures plugin overhead)
- **Dagger mode** (`--mode dagger`): Uses Dagger with Anvil for dependency injection
- **Kotlin-inject + Anvil mode** (`--mode kotlin-inject-anvil`): Uses Metro with kotlin-inject + anvil interop
- **Module count** (`--count <number>`): Total number of modules to generate (default: 500)
- **Processor** (`--processor ksp|kapt`): Annotation processor for Dagger mode (default: ksp)
- **Provider multibindings** (`--provider-multibindings`): Wrap multibinding accessors in `Provider<Set<E>>` instead of `Set<E>` to benchmark `SetFactory`/`MapFactory` optimizations

## Usage

```bash
# Generate the project for Metro mode with default 500 modules
kotlin generate-projects.main.kts --mode metro

# Generate the project for Dagger mode with default 500 modules
kotlin generate-projects.main.kts --mode dagger

# Generate Vanilla baseline project (pure Kotlin, no DI)
kotlin generate-projects.main.kts --mode vanilla

# Generate Metro-NOOP project (Metro plugin applied but no annotations)
kotlin generate-projects.main.kts --mode metro_noop

# Generate a larger project with 1000 modules
kotlin generate-projects.main.kts --mode metro --count 1000

# Generate a smaller project for quick testing
kotlin generate-projects.main.kts --mode dagger --count 100

# Generate a project using kapt for dagger-compiler (Dagger mode)
kotlin generate-projects.main.kts --mode dagger --processor kapt

# Generate kotlin-inject + anvil mode project (uses Amazon kotlin-inject-anvil)
kotlin generate-projects.main.kts --mode kotlin-inject-anvil

# Generate with Provider-wrapped multibindings (for SetFactory/MapFactory benchmarking)
kotlin generate-projects.main.kts --mode metro --provider-multibindings

# Build the entire benchmark
./gradlew build

# Run the app component (creates the full dependency graph)
./gradlew :app:component:run
```

## Benchmark Runner

Use the `run_benchmarks.sh` script for comprehensive performance testing:

```bash
# Run all benchmark modes on current branch (metro, dagger-ksp, dagger-kapt, kotlin-inject-anvil)
./run_benchmarks.sh all

# Run all modes including baseline benchmarks (vanilla + metro-noop)
./run_benchmarks.sh all --include-baselines

# Run specific modes on current branch
./run_benchmarks.sh metro 500
./run_benchmarks.sh dagger-ksp 250
./run_benchmarks.sh kotlin-inject-anvil 500

# Include clean build scenarios (opt-in)
./run_benchmarks.sh all --include-clean-builds

# Using single/compare commands for more control
./run_benchmarks.sh single --ref HEAD --modes metro,dagger-ksp   # Current branch
./run_benchmarks.sh single --ref main --modes all                 # Specific branch
./run_benchmarks.sh single --ref feature-branch --modes metro     # Feature branch
./run_benchmarks.sh compare --ref1 main --ref2 feature-branch --modes all

# Results are saved to timestamped directories in benchmark-results/
# HTML reports are generated with comparison charts
```

### Benchmark Scenarios

The benchmark suite uses generic scenarios that are shared across all modes. The script automatically
generates the appropriate project for each mode and runs the same scenarios against it.

**Standard Scenarios (always included):**
- **ABI Change**: Measures incremental compilation when public API changes (DI-annotated files)
- **Non-ABI Change**: Measures incremental compilation when implementation changes (DI-annotated files)
- **Plain Kotlin ABI/Non-ABI Change**: Same as above but for plain Kotlin files without DI annotations
- **Raw Compilation**: Measures full compilation performance of `:app:component`
  - **Metro/Vanilla/Metro-NOOP**: Pure compiler plugin, uses `--rerun-tasks` on `compileKotlin`
  - **kotlin-inject-anvil**: KSP generates Kotlin, uses `--rerun-tasks -a` on `compileKotlin`
  - **Dagger (KSP/KAPT)**: Generates Java code, uses `--rerun-tasks -a` on `classes` task

**Clean Build Scenarios (opt-in with `--include-clean-builds`):**
- **Clean Build**: Measures full compilation from scratch with no caches
  - Uses `cleanup-tasks = ["clean"]` to run clean before each iteration
  - Uses `clear-build-cache-before = BUILD` to clear Gradle build cache
  - Lower iteration count (3) and warm-ups (2) due to longer execution time

Clean build scenarios are useful for:
- Measuring cold build performance
- Testing CI/ephemeral build scenarios
- Comparing full compilation times across different DI frameworks

**Scenario Implementation:**

The `benchmark.scenarios` file contains generic scenarios that are selected based on mode:
- `abi_change` / `abi_change_ksp` - ABI change scenarios (KSP variant uses `--rerun-tasks` for fair comparison)
- `non_abi_change`, `plain_abi_change`, `plain_non_abi_change` - Incremental scenarios
- `raw_compilation` (Metro/Vanilla/Metro-NOOP), `raw_compilation_ksp` (kotlin-inject-anvil), `raw_compilation_java` (Dagger)
- `clean_build`

**Note on KSP incremental behavior:** KSP's incremental model may skip work when only the classpath changes
(e.g., modifying an interface in a dependency). For kotlin-inject-anvil, we use `--rerun-tasks` for ABI
change scenarios to ensure KSP regenerates the merged component, providing a fair comparison to Metro's
compiler plugin which always runs during `compileKotlin`.

The `run_benchmarks.sh` script handles mode-specific logic by regenerating the project for each mode
and selecting the appropriate scenario variant based on the mode's compilation requirements.

## Modes

### Metro Mode
- Uses `dev.zacsweers.metro` plugin
- Uses `dev.zacsweers.anvil:annotations` with Metro interop
- Supports `AppScope` and `@SingleIn` scoping
- Generates `createGraph<AppComponent>()` for runtime execution

### Vanilla Mode (Baseline)
- Pure Kotlin with no DI framework at all
- No compiler plugins applied
- Generates the same class structure as other modes but without DI annotations
- Provides a true baseline for measuring Kotlin compilation overhead
- Useful for determining how much overhead DI frameworks add

### Metro-NOOP Mode (Plugin Overhead)
- Metro compiler plugin is applied but no Metro annotations are used
- Generates the same class structure as Vanilla mode (no DI annotations)
- Measures the overhead of having the Metro plugin present with nothing to process
- Useful for understanding Metro's plugin baseline cost

### Dagger + Anvil Mode
- Uses `dagger-compiler` with KSP or KAPT for component/factory generation
- Uses `dev.zacsweers.anvil:compiler` with KSP for contribution merging
- Uses `dagger.runtime` for dependency injection
- Uses standard `com.squareup.anvil.annotations` (via anvil-ksp fork)
- Uses `@Singleton` and `Unit::class` scope
- Generates Dagger components with `DaggerAppComponent.factory().create()`

### Kotlin-inject + Anvil Mode
- Uses `me.tatarka.inject:kotlin-inject-runtime` and `kotlin-inject-compiler-ksp` for dependency injection
- Uses `software.amazon.lastmile.kotlin.inject.anvil` compiler and runtime for code generation
- Uses `@SingleIn(AppScope::class)` scoping and `@ContributesMultibinding` for multibindings
- Uses `me.tatarka.inject.annotations.Inject` for dependency injection
- Uses abstract class with `@Component` and `@MergeComponent` pattern with `AppComponent::class.create()`

## Interop

The benchmark uses:

- `javax.inject` annotations for dependency injection
- `dev.zacsweers.anvil:annotations` (anvil-ksp fork) for contribution annotations
- Metro's interop system to bridge Anvil annotations to Metro's code generation

This tests Metro's ability to work with existing Anvil codebases and validates the interop functionality at scale.

## Statistics

- **Configurable module count** (default: 500, can scale from 100 to 1000+)
- **~5.5 contributions per module** (varies due to randomization, 1-10 range)
- **Proportional subcomponents** (~10% of app layer modules)
- **Realistic dependency graph** following architectural best practices

The benchmark scales proportionally with the module count:
- 100 modules: ~500 contributions, 1 subcomponent
- 500 modules: ~2,750 contributions, 7 subcomponents
- 1000 modules: ~5,400 contributions, 14 subcomponents

This provides comprehensive testing of Metro's compilation performance, memory usage, and incremental
compilation behavior across different project scales.

## Startup Benchmarks

In addition to compilation benchmarks, this project includes startup (runtime) benchmarks that measure
the time to create and initialize Metro dependency graphs. Both JVM and Android benchmarks use the
same generated `AppComponent` via the `createAndInitialize()` function.

### JVM Startup Benchmarks

Uses [JMH](https://github.com/openjdk/jmh) (Java Microbenchmark Harness) to measure graph creation time on JVM.

```bash
# First, generate the benchmark project if not already done
kotlin generate-projects.main.kts --mode metro

# Run JMH benchmarks
./gradlew :startup-jvm:jmh

# Results are saved to startup-jvm/build/results/jmh/
```

The JVM benchmark measures `graphCreationAndInitialization` - the time to create the Metro dependency
graph and fully initialize it by accessing all multibindings.

### JVM Startup Benchmarks (R8 Minified)

For measuring performance of optimized/minified builds, there's also an R8-minified JVM benchmark. This
runs the same JMH benchmarks but against an R8-processed jar that simulates production Android builds.

```bash
# Run JMH benchmarks with R8-minified Metro classes
./gradlew :startup-jvm-minified:jmh

# Results are saved to startup-jvm-minified/build/results/jmh/
```

The R8 benchmark uses the `:startup-jvm:minified-jar` module which:
- Takes the `:app:component` classes and all dependencies
- Runs R8 with optimization flags (`-allowaccessmodification`, `-repackageclasses`)
- Keeps only the `createAndInitialize()` entry point and `AppComponent` interface

The `:startup-jvm-minified` module then depends on this minified jar and runs standard JMH benchmarks.

This provides insight into how Metro performs in production Android apps where R8 is typically enabled.

### Multiplatform Startup Benchmarks (kotlinx-benchmark)

Uses [kotlinx-benchmark](https://github.com/Kotlin/kotlinx-benchmark) to measure graph creation
performance across multiple Kotlin targets (JVM, JS, WasmJS, Native).

```bash
# First, generate the multiplatform benchmark project
kotlin generate-projects.main.kts --mode metro --multiplatform

# Run kotlinx-benchmark for all targets
./gradlew :startup-multiplatform:benchmark

# Run for specific targets
./gradlew :startup-multiplatform:jvmBenchmark
./gradlew :startup-multiplatform:jsBenchmark
./gradlew :startup-multiplatform:wasmJsBenchmark
./gradlew :startup-multiplatform:macosArm64Benchmark  # or macosX64Benchmark, linuxX64Benchmark

# Results are saved to startup-multiplatform/build/reports/benchmarks/
```

Or use the runner script:

```bash
# Run all targets
./run_startup_benchmarks.sh multiplatform

# Run specific target
./run_startup_benchmarks.sh multiplatform --target jvm
./run_startup_benchmarks.sh multiplatform --target js
./run_startup_benchmarks.sh multiplatform --target wasmJs
./run_startup_benchmarks.sh multiplatform --target native

# Results are saved to startup-benchmark-results/{timestamp}/multiplatform-{target}_metro/
```

#### Analyzing Results with Kotlin Notebooks

kotlinx-benchmark outputs JSON results that can be analyzed in [Kotlin Notebooks](https://plugins.jetbrains.com/plugin/16340-kotlin-notebook)
for statistical analysis and visualization.

1. Install the Kotlin Notebook plugin in IntelliJ IDEA
2. Open `notebooks/analyze-benchmark-results.ipynb`
3. Update the results path and run cells

See [JetBrains Blog: Exploring kotlinx-benchmark Results](https://blog.jetbrains.com/kotlin/2025/12/a-better-way-to-explore-kotlinx-benchmark-results-with-kotlin-notebooks/)
for more details on analysis techniques.

### Android Startup Benchmarks

Uses [AndroidX Macrobenchmark](https://developer.android.com/topic/performance/benchmarking/macrobenchmark-overview)
to measure Metro's impact on Android app startup time. The app calls `createAndInitialize()` in
`Application.onCreate()`, so the benchmark measures realistic startup performance.

```bash
# Build and run benchmarks (requires connected device/emulator)
./gradlew :startup-android:app:assembleBenchmark
./gradlew :startup-android:benchmark:connectedBenchmarkAndroidTest

# Results are saved to startup-android/benchmark/build/outputs/connected_android_test_additional_output/
```

The benchmark measures cold startup - when the app is launched after being killed. This includes all
class loading, Metro graph creation, and initialization.

### Running All Startup Benchmarks

Use the `run_startup_benchmarks.sh` script to run all startup benchmarks and aggregate results:

```bash
# Run all startup benchmarks (includes JVM, JVM-R8, and Android)
./run_startup_benchmarks.sh all

# Run only JVM benchmarks
./run_startup_benchmarks.sh jvm

# Run only JVM R8-minified benchmarks (Metro only)
./run_startup_benchmarks.sh jvm-r8

# Run multiplatform benchmarks (Metro only, kotlinx-benchmark)
./run_startup_benchmarks.sh multiplatform
./run_startup_benchmarks.sh multiplatform --target jvm  # specific target

# Run only Android benchmarks (requires device)
./run_startup_benchmarks.sh android

# Results are saved to startup-benchmark-results/
```

### Binary Metrics Only Mode

For quick testing of binary metrics (class analysis, JAR sizes, APK sizes) without running full JMH/device benchmarks:

```bash
# Build and extract binary metrics only (skips JMH and device benchmarks)
./run_startup_benchmarks.sh single --ref main --modes metro --count 500 --binary-metrics-only

# Compare binary metrics between two git refs
./run_startup_benchmarks.sh compare --ref1 main --ref2 feature-branch --modes metro --binary-metrics-only

# Run only JVM-R8 binary metrics
./run_startup_benchmarks.sh single --ref main --modes metro --benchmark jvm-r8 --binary-metrics-only

# Useful for quickly comparing:
# - Pre-minification class metrics (fields, methods, shards, size)
# - R8-minified JAR sizes
# - Android APK sizes and diffuse analysis
```

This mode is also available in the GitHub Actions workflow via the `binary-metrics-only` checkbox.

### Using Current Working State

You can benchmark the current working state (including uncommitted changes) without needing to commit or stash:

```bash
# Benchmark current state (includes uncommitted changes)
./run_startup_benchmarks.sh single --ref HEAD --modes metro

# Also accepts 'head' or 'current' as aliases
./run_startup_benchmarks.sh single --ref current --modes metro --binary-metrics-only
```

This is useful for quickly testing local changes before committing.

### Benchmarking Published Metro Versions

You can benchmark against published Metro versions from Maven Central instead of the included build. The scripts automatically detect semantic version strings and configure Gradle accordingly.

```bash
# Benchmark a specific Metro version
./run_startup_benchmarks.sh single --ref 1.0.0 --modes metro

# Compare two published versions
./run_benchmarks.sh compare --ref1 1.0.0 --ref2 1.1.0 --modes metro

# Compare a published version against a git branch
./run_startup_benchmarks.sh compare --ref1 1.0.0 --ref2 main --modes metro

# Works with pre-release versions too
./run_startup_benchmarks.sh single --ref 2.0.0-alpha01 --modes metro
```

**How it works:**

The scripts detect Metro versions using a semantic version pattern (e.g., `1.0.0`, `2.0.0-alpha01`, `1.5.0-RC1`). When a Metro version is detected:

1. The `METRO_VERSION` environment variable is set
2. Git checkout is skipped (benchmarks run on current branch)
3. The `settings.gradle.kts` picks up `METRO_VERSION` and:
   - Overrides the metro version in the version catalog
   - Skips `includeBuild("..")` so Gradle fetches from Maven Central instead

This allows comparing performance across Metro releases or testing a published version against local changes.

### Android App Configuration

The Android benchmark app (`startup-android/app`) is configured with:
- **R8 optimization**: Minification and shrinking enabled for release/benchmark builds

## Continuous Regression Testing

Automated benchmark regression testing runs via GitHub Actions to catch performance regressions early.

### How It Works

The `benchmark-regression.yml` workflow uses a **paired comparison** approach:
- On every push to `main`, it benchmarks both `HEAD~1` (baseline) and `HEAD` (current)
- Both benchmarks run on the **same machine** in the same job
- This eliminates hardware variance - only the delta between runs matters

```
─── baseline (main~1)    ─── current (main)

     │
0.20 │    ●────●              ← Both jump (different machine)
     │   /      \
0.15 │  ●        ●────●       ← Both drop (different machine)
     │ ╱          ╲   ╲
0.10 │●            ●───●
     └─────────────────────
       run1  run2  run3  run4

The gap between lines = real performance change
Both lines moving together = hardware variance (ignore)
```

### What Gets Benchmarked

Two benchmarks run in parallel:

| Benchmark | What it measures | Tool |
|-----------|------------------|------|
| **Startup** | Time to create and initialize Metro graph | kotlinx-benchmark |
| **Build** | Time to compile the benchmark project | Gradle Profiler |

### Triggering Benchmarks

**Automatic (on push to main):**
- Runs when `compiler/`, `runtime/`, or `benchmark/` paths change
- Results are stored for historical tracking

**Manual (on PRs):**
1. Add the `benchmark` label to your PR
2. Workflow compares your PR against `main`
3. Comments on the PR if >5% regression detected

### Viewing Results

**Historical Charts:**
- Visit [zacsweers.github.io/metro/dev/bench/](https://zacsweers.github.io/metro/dev/bench/) for startup benchmarks
- Visit [zacsweers.github.io/metro/dev/bench/build/](https://zacsweers.github.io/metro/dev/bench/build/) for build time benchmarks

**Per-Run Results:**
- Check the workflow run's **Summary** tab for a comparison table
- Download artifacts for raw JSON/CSV data

### Regression Threshold

A **5% slowdown** triggers a regression warning. This threshold balances:
- Catching meaningful regressions
- Avoiding false positives from measurement noise

### Manual Benchmarks

For more comprehensive benchmarking (multiple modes, Android, etc.), use the manual `benchmarks.yml` workflow:

```bash
# Via GitHub CLI
gh workflow run benchmarks.yml \
  -f ref1=main \
  -f ref2=feature-branch \
  -f metro=true \
  -f run-multiplatform-benchmarks=true
```

Or trigger from the Actions tab in GitHub with the desired options.
